No CUDA runtime is found, using CUDA_HOME='/rdrive/ref/cuda/lin/cuda-12.4'
2.1.0.post2+cxx11.abi
2.1.30.post0
[0]: _DeviceProperties(name='Intel(R) Data Center GPU Max 1100', platform_name='Intel(R) Level-Zero', dev_type='gpu', driver_version='1.3.29138', has_fp64=1, total_memory=49152MB, max_compute_units=448, gpu_eu_count=448)
models/xgen-7b-8k-base
name1  model.layers.0.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.0.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.0.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.0.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.0.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.0.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.0.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.1.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.1.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.1.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.1.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.1.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.1.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.1.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.2.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.2.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.2.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.2.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.2.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.2.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.2.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.3.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.3.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.3.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.3.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.3.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.3.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.3.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.4.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.4.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.4.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.4.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.4.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.4.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.4.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.5.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.5.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.5.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.5.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.5.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.5.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.5.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.6.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.6.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.6.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.6.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.6.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.6.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.6.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.7.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.7.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.7.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.7.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.7.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.7.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.7.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.8.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.8.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.8.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.8.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.8.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.8.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.8.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.9.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.9.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.9.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.9.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.9.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.9.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.9.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.10.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.10.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.10.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.10.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.10.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.10.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.10.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.11.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.11.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.11.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.11.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.11.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.11.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.11.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.12.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.12.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.12.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.12.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.12.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.12.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.12.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.13.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.13.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.13.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.13.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.13.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.13.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.13.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.14.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.14.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.14.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.14.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.14.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.14.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.14.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.15.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.15.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.15.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.15.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.15.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.15.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.15.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.16.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.16.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.16.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.16.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.16.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.16.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.16.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.17.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.17.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.17.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.17.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.17.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.17.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.17.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.18.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.18.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.18.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.18.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.18.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.18.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.18.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.19.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.19.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.19.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.19.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.19.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.19.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.19.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.20.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.20.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.20.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.20.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.20.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.20.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.20.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.21.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.21.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.21.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.21.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.21.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.21.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.21.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.22.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.22.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.22.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.22.self_attn.v_proj
2024-06-25 04:12:17,194 - datasets - INFO - PyTorch version 2.1.0.post2+cxx11.abi available.
Using unk_token, but it is not set yet.
Using unk_token, but it is not set yet.
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.22.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.22.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.22.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.23.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.23.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.23.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.23.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.23.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.23.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.23.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.24.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.24.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.24.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.24.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.24.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.24.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.24.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.25.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.25.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.25.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.25.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.25.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.25.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.25.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.26.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.26.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.26.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.26.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.26.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.26.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.26.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.27.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.27.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.27.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.27.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.27.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.27.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.27.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.28.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.28.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.28.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.28.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.28.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.28.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.28.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.29.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.29.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.29.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.29.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.29.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.29.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.29.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.30.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.30.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.30.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.30.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.30.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.30.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.30.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.31.self_attn.k_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.31.self_attn.o_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.31.self_attn.q_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.31.self_attn.v_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.31.mlp.down_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.31.mlp.gate_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
name1  model.layers.31.mlp.up_proj
self.rows:  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)
Loading model ...
Done.
Model type : llama
Benchmarking ...
0 0.44020581245422363
1 0.04612326622009277
2 0.041541099548339844
3 0.04154610633850098
4 0.04142045974731445
5 0.041368961334228516
6 0.04118943214416504
7 0.04118633270263672
8 0.041288137435913086
9 0.05557394027709961
10 0.0412445068359375
11 0.04139828681945801
12 0.04173469543457031
13 0.04138803482055664
14 0.04142022132873535
15 0.04173779487609863
16 0.041886091232299805
17 0.041800737380981445
18 0.04137849807739258
19 0.04151797294616699
20 0.04177999496459961
21 0.04170370101928711
22 0.04174232482910156
23 0.041867971420288086
24 0.04162287712097168
25 0.04181361198425293
26 0.0418093204498291
27 0.04176592826843262
28 0.04169154167175293
29 0.04188179969787598
30 0.04150986671447754
31 0.04156994819641113
32 0.04142141342163086
33 0.04157233238220215
34 0.04146456718444824
35 0.041347503662109375
36 0.041402339935302734
37 0.041248321533203125
38 0.04138445854187012
39 0.041382789611816406
40 0.04153561592102051
41 0.04158926010131836
42 0.04153633117675781
43 0.041372060775756836
44 0.041425228118896484
45 0.041686296463012695
46 0.04145383834838867
47 0.04169416427612305
48 0.041663408279418945
49 0.041536808013916016
50 0.041589975357055664
51 0.04176068305969238
52 0.0418696403503418
53 0.041788578033447266
54 0.04169440269470215
55 0.04152822494506836
56 0.04142141342163086
57 0.04150056838989258
58 0.04188823699951172
59 0.04186725616455078
60 0.0416562557220459
61 0.041506290435791016
62 0.041518211364746094
63 0.04172348976135254
64 0.04197978973388672
65 0.04172682762145996
66 0.04158926010131836
67 0.04202890396118164
68 0.041676998138427734
69 0.04181480407714844
70 0.04160189628601074
71 0.04158210754394531
72 0.04175543785095215
73 0.041602373123168945
74 0.04183554649353027
75 0.04178881645202637
76 0.04170393943786621
77 0.04185175895690918
78 0.041741132736206055
79 0.04172492027282715
80 0.041797637939453125
81 0.04165983200073242
82 0.04204154014587402
83 0.04221940040588379
84 0.04182767868041992
85 0.04204511642456055
86 0.0419764518737793
87 0.04203677177429199
88 0.04175162315368652
89 0.04193878173828125
90 0.04193878173828125
91 0.04196476936340332
92 0.041960954666137695
93 0.042243242263793945
94 0.04207730293273926
95 0.04188251495361328
96 0.04197812080383301
97 0.0419619083404541
98 0.04194784164428711
99 0.042356014251708984
100 0.04208970069885254
101 0.042157888412475586
102 0.042276859283447266
103 0.04188704490661621
104 0.04191470146179199
105 0.04240822792053223
106 0.04221630096435547
107 0.04188656806945801
108 0.04213309288024902
109 0.04210376739501953
110 0.04217028617858887
111 0.042118072509765625
112 0.04219865798950195
113 0.04211068153381348
114 0.042101383209228516
115 0.041983842849731445
116 0.04207658767700195
117 0.04222464561462402
118 0.04225516319274902
119 0.04216337203979492
120 0.042105674743652344
121 0.04243659973144531
122 0.042493343353271484
123 0.0426638126373291
124 0.0425722599029541
125 0.04237651824951172
126 0.042172908782958984
127 0.04601740837097168
Median: 0.041788697242736816
PPL: 160932.125
max memory(MiB): 4629.5302734375
